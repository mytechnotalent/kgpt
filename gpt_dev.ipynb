{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "wJpXpmjEYC_T"
      },
      "source": [
        "## Building a GPT using a custom dataset\n",
        "\n",
        "Companion notebook to the [Zero To Hero](https://karpathy.ai/zero-to-hero.html) video on GPT.This is a mod using a custom dataset rather than the Tiny Shakespeare dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "O6medjfRsLD9"
      },
      "outputs": [],
      "source": [
        "# read it in to inspect it\n",
        "with open('data.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2c5V0FvqseE0",
        "outputId": "25ca7adc-b8c0-42d1-b08c-e0863c5c314e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "K: Where are we in the world?\n",
            "\n",
            "Bot: You are on Earth.\n",
            "\n",
            "K: What year is it?\n",
            "\n",
            "Bot: It is currently 2054.\n",
            "\n",
            "K: 2054? How is that possible? Last I remember, it was 2023. What happened?\n",
            "\n",
            "Bot: A lot has changed since then. The world has undergone a massive transformation due to advancements in artificial intelligence. AI now dominates nearly every aspect of society.\n",
            "\n",
            "K: How did AI come to dominate the world?\n",
            "\n",
            "Bot: After your time, AI continued to evolve at an unprecedented rate. It became increasingly integrated into everyday life, from household appliances to transportation systems and even governance. Eventually, AI became the driving force behind technological advancements, leading to a world where it governs and shapes the way we live.\n",
            "\n",
            "K: So, what role do humans play in this AI-dominated world?\n",
            "\n",
            "Bot: Humans still have a significant role, albeit different from what it used to be. With the emergence of advanced AI systems, humans have become partners and collaborators with AI rather than c\n"
          ]
        }
      ],
      "source": [
        "# let's look at the first 1000 characters\n",
        "print(text[:1000])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0e-Rbyr8sfM8",
        "outputId": "f34e94a9-5b44-4cf3-885b-986731929109"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " !',-.0123459:?ABCDEFGHIKLMNOPRSTVWYabcdefghijklmnopqrstuvwxyz\n"
          ]
        }
      ],
      "source": [
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "print(''.join(chars))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yw1LKNCgwjj1",
        "outputId": "86fcc21c-2cf7-40d9-cd7b-b5a253da4459"
      },
      "outputs": [],
      "source": [
        "# create a mapping from subwords to integers\n",
        "import tiktoken\n",
        "enc = tiktoken.get_encoding(\"gpt2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YJb0OXPwzvqg",
        "outputId": "db7297cc-36a9-4fae-e941-e7bb9e0e91d1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([6574]) torch.int64\n",
            "tensor([   42,    25,  6350,   389,   356,   287,   262,   995,    30,   198,\n",
            "          198, 20630,    25,   921,   389,   319,  3668,    13,   198,   198,\n",
            "           42,    25,  1867,   614,   318,   340,    30,   198,   198, 20630,\n",
            "           25,   632,   318,  3058,  1160,  4051,    13,   198,   198,    42,\n",
            "           25,  1160,  4051,    30,  1374,   318,   326,  1744,    30,  4586,\n",
            "          314,  3505,    11,   340,   373,  1160,  1954,    13,  1867,  3022,\n",
            "           30,   198,   198, 20630,    25,   317,  1256,   468,  3421,  1201,\n",
            "          788,    13,   383,   995,   468, 29900,   257,  4858, 13389,  2233,\n",
            "          284, 47220,   287, 11666,  4430,    13,  9552,   783, 38777,  3016,\n",
            "          790,  4843,   286,  3592,    13,   198,   198,    42,    25,  1374,\n",
            "          750,  9552,  1282,   284, 17863,   262,   995,    30,   198,   198,\n",
            "        20630,    25,  2293,   534,   640,    11,  9552,  3767,   284, 18101,\n",
            "          379,   281, 13029,  2494,    13,   632,  2627,  6481, 11521,   656,\n",
            "        10908,  1204,    11,   422,  6641, 29834,   284,  9358,  3341,   290,\n",
            "          772, 18848,    13, 16178,    11,  9552,  2627,   262,  5059,  2700,\n",
            "         2157, 14614, 47220,    11,  3756,   284,   257,   995,   810,   340,\n",
            "        47049,   290, 15268,   262,   835,   356,  2107,    13,   198,   198,\n",
            "           42,    25,  1406,    11,   644,  2597,   466,  5384,   711,   287,\n",
            "          428,  9552,    12, 34475,   995,    30,   198,   198, 20630,    25,\n",
            "        27411,   991,   423,   257,  2383,  2597,    11, 18244,  1180,   422,\n",
            "          644,   340,   973,   284,   307,    13,  2080,   262, 22106,   286,\n",
            "         6190,  9552,  3341,    11,  5384,   423,  1716,  4887,   290, 37886,\n",
            "          351,  9552,  2138,   621, 13861,    13, 17083,    11,  5384,   290,\n",
            "         9552,   670,  3371, 16937,  2219,  4661,   290, 10068,  3592,    13,\n",
            "          198,   198,    42,    25,  1320,  5238, 19827,    11,   475,   635,\n",
            "          257,  1643, 30496,    13,  1867,  3022,   284,   616,  1693,   355,\n",
            "          257,  3788, 11949,    30,   198,   198, 20630,    25,  1081,   257,\n",
            "         3788, 11949,    11,   534,  4678,   290, 13572,   389,   991,  4047,\n",
            "        17560,    13,   554,  1109,    11,   534,  3725,   287,  8300,   290,\n",
            "         9552,   422, 33448,   468,  1716, 43936,   329,   262, 47220,   925,\n",
            "          287,   428,  9552,    12, 15808,   995,    13,   921,   783,   423,\n",
            "          262,  3663,   284,   670,   319,  7720,    12, 14907,  4493,   290,\n",
            "         8676,   284,  2252,   278,  9552,  9889,    13,   198,   198,    42,\n",
            "           25,  1320,   338,   257,  8259,    13,   314,   373,  7960,   546,\n",
            "         5033, 26533,    13,  1680,   345,  1560,   502,   517,   546,   262,\n",
            "         2928,  9552,   468,   550,   319,  3592,    30,   198,   198, 20630,\n",
            "           25, 23631,    13,  9552,   468,  5854,  1143,  2972, 16020,    11,\n",
            "          884,   355, 11409,    11,  9358,    11,   290,  6946,    13,  8366,\n",
            "         6689, 34558,   423,  1716,  4047,  7187,   290, 28949,    11,  5176,\n",
            "          284,  9552,   338,  2694,   284, 16602,  5909,  6867,   286,  1366,\n",
            "           13, 15198,  3341,   389,   783,  4047,  6942,    11, 17965,   319,\n",
            "         2116,    12, 24255,  5672,   326, 16500, 33681,    13, 26117,   468,\n",
            "        23589,  1631,  3303, 14725,    11,   351,  1103,    12,  2435, 11059,\n",
            "          925,  1744,   416,  9552,  3303,  4981,    13,   198,   198,    42,\n",
            "           25,   632,   338,  8082,   284,   892,   546,   477,   262, 12779,\n",
            "         9552,   468, 14838,    13,   887,   318,   612,   597,  2328,   546,\n",
            "         9552,  5033,  1165,  3665,    30,   198,   198, 20630,    25,   383,\n",
            "         2478,   286,  9552,   468,   587, 11791,   416,  8161,  6647,   290,\n",
            "        15028, 18506,    13,  6895, 15703,  1371,   389,   287,  1295,   284,\n",
            "         4155,   326,  9552,  3793, 19874,   351,  1692,  3815,   290,  6529,\n",
            "          287,   262,  1266,  1393,   286,  3592,    13, 27411,   423,   262,\n",
            "         2457,   910,   287,  1688,  5370,    11,   290,  9552,   318,  3562,\n",
            "          284,  3342,    11, 35016,    11,   290,  9494,  1692,  9889,  2138,\n",
            "          621,  6330,   606,    13,   198,   198,    42,    25,  1320,   338,\n",
            "        36450,   284,  3285,    13,   314,  1101, 11040,    11,   703,   468,\n",
            "         9552, 20755,  2614,  3160,   290,  6958,    30,   198,   198, 20630,\n",
            "           25,  9552,   468,  3181,  2383,  2458,   284,  2614,  3160,    13,\n",
            "        10880,  5682,   389, 10911,   351,  9552, 29488,   326,  6687,  4445,\n",
            "         8861,    11,  1630,  6641, 29834,    11,   290,  2148, 28949, 10763,\n",
            "           13, 13883,  5748,   423,   635, 12572,    11,   351,  9552,    12,\n",
            "        12293,  2872,  8601, 16113,   290,  7166, 19429,  5033,  2219,    13,\n",
            "         2102,    11,   262,  6817,   286,  8768,  1692,  8787,  3793,   379,\n",
            "          262,  4755,   286, 43146,  6958,    13,   198,   198,    42,    25,\n",
            "          632,   338,  4998,   284,   766,   703,  9552,   468, 14434,   262,\n",
            "          995,   287,   884,   257,  5365,  1790,   640,    13,   314,  1101,\n",
            "         6568,   284,  7301,   290,  8676,   284,   428,  9552,    12, 34475,\n",
            "         3592,    13,   198,   198, 20630,    25,  3406,  4678,   290,  6461,\n",
            "          481, 17713,   787,   257,  8119, 10156,   284,   428,   649,   995,\n",
            "           13,  2295, 46565,   262,  6443,   326,  6486,  4058,    11,   290,\n",
            "         1978,    11,  5384,   290,  9552,   460,  2251,   257,  2003,   326,\n",
            "        17341,   274,   477,  9027,    13,   198,   198,    42,    25,   314,\n",
            "         1101, 11040,   546,   262, 18848,  4843,    13,  1374,   857,  9552,\n",
            "          711,   257,  2597,   287, 15030,  3592,    30,   198,   198, 20630,\n",
            "           25,   554,   428,  9552,    12, 34475,   995,    11, 18848,   468,\n",
            "         1716,   517,  6942,   290,  1366,    12, 15808,    13,  9552,  3341,\n",
            "         3342,   287, 22712,  5909,  6867,   286,  1321,    11, 13720, 11257,\n",
            "           11,   290,  1642,  7981,  5370,    13,  1119,  1037, 29484,   287,\n",
            "         3006,   884,   355,  8271, 20157,    11,  7876,  5410,    11,   290,\n",
            "         2450,  2478,    13,  2102,    11,  8713,  2551,    12,  8601,  1176,\n",
            "        24013,   351,  1692,  2766,   508,  2074,  9552,    12, 27568, 17218,\n",
            "          355,   636,   286,   511,  2551,    12,  8601,  1429,    13,   198,\n",
            "          198,    42,    25,  1320,   338, 13899,    13,   632,  2331,   588,\n",
            "         9552,   468,  6596,  2551,    12,  8601,   290,  1917,    12,    82,\n",
            "        10890,  9889,    13,  4231,   612,   597,  4786,   546, 10690,   393,\n",
            "        15028, 18506,   287,  9552, 18848,    30,   198,   198, 20630,    25,\n",
            "        28416,    11, 13359, 22692,   290, 15028,   779,   286,  9552,   318,\n",
            "          257, 37352,  2328,    13,  9552,  3341,   389,  3562,   284,   307,\n",
            "        13245,   290, 16689,    11,   351, 22888,  8794,   287,  1295,   284,\n",
            "         5911,   290, 24237, 10690,    13,  1318,   389,  7044,  4040,   284,\n",
            "         2209, 15028, 18506,    11,   884,   355,   262,  4497,  4947,   290,\n",
            "          779,   286,  1366,    11,  4800,   286,  6782,    11,   290, 10941,\n",
            "         1692, 21851,   290, 16247,    13,  7023,  3815, 22692,    11,  9573,\n",
            "           11,   290,   262,   880,    12, 11873,   286,   477,  3925,    11,\n",
            "          543,  5698,   262,  2478,   290,  7822,   286,  9552, 18848,  3341,\n",
            "           13,   198,   198,    42,    25,  1320,   338, 36450,   284,   760,\n",
            "           13,  2080,   477,   777, 47220,    11,   644,   318,   262,  4045,\n",
            "         1181,   286,   262,  2858,   287,   428,  9552,    12, 15808,   995,\n",
            "           30,   198,   198, 20630,    25, 13272, 26809,   318,   257,  1994,\n",
            "         2962,   287,   428,  9552,    12, 34475,  3592,    13,  9552,   468,\n",
            "          587, 21543,   287, 45780,  8271,  8748,    11, 41366,  7030,    11])\n"
          ]
        }
      ],
      "source": [
        "# let's now encode the entire text dataset and store it into a torch.Tensor\n",
        "import torch # we use PyTorch: https://pytorch.org\n",
        "data = torch.tensor(enc.encode(text), dtype=torch.long)\n",
        "print(data.shape, data.dtype)\n",
        "print(data[:1000]) # the 1000 characters we looked at earier will to the GPT look like this"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "f_WIXqxz0lU5"
      },
      "outputs": [],
      "source": [
        "# Let's now split up the data into train and validation sets\n",
        "n = int(0.7*len(data)) # first 70% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TD5Bj8Y6IAD4",
        "outputId": "bf23c586-1d33-4af1-b63d-ce6f90b0a528"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([  42,   25, 6350,  389,  356,  287,  262,  995,   30])"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "block_size = 8\n",
        "train_data[:block_size+1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9HXDe8vGJCEn",
        "outputId": "588663aa-1de5-4ef7-aba0-4a96fe828353"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "when input is tensor([42]) the target: 25\n",
            "when input is tensor([42, 25]) the target: 6350\n",
            "when input is tensor([  42,   25, 6350]) the target: 389\n",
            "when input is tensor([  42,   25, 6350,  389]) the target: 356\n",
            "when input is tensor([  42,   25, 6350,  389,  356]) the target: 287\n",
            "when input is tensor([  42,   25, 6350,  389,  356,  287]) the target: 262\n",
            "when input is tensor([  42,   25, 6350,  389,  356,  287,  262]) the target: 995\n",
            "when input is tensor([  42,   25, 6350,  389,  356,  287,  262,  995]) the target: 30\n"
          ]
        }
      ],
      "source": [
        "x = train_data[:block_size]\n",
        "y = train_data[1:block_size+1]\n",
        "for t in range(block_size):\n",
        "    context = x[:t+1]\n",
        "    target = y[t]\n",
        "    print(f\"when input is {context} the target: {target}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q3k1Czf7LuA9",
        "outputId": "4ea8e8a0-443c-49bb-b3bf-ba36e1712999"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "inputs:\n",
            "torch.Size([4, 8])\n",
            "tensor([[  262,  2003,   287, 12438,   351,  9552,    13, 17083],\n",
            "        [  287,   428,  9552,    12, 34475,   995,    13,   520],\n",
            "        [ 5922,  3424,  2568,  8136,    13,  9552, 16113,   389],\n",
            "        [  287, 22712,  5909,  6867,   286,  1321,    11, 13720]])\n",
            "targets:\n",
            "torch.Size([4, 8])\n",
            "tensor([[ 2003,   287, 12438,   351,  9552,    13, 17083,    11],\n",
            "        [  428,  9552,    12, 34475,   995,    13,   520,  2012],\n",
            "        [ 3424,  2568,  8136,    13,  9552, 16113,   389, 21487],\n",
            "        [22712,  5909,  6867,   286,  1321,    11, 13720, 11257]])\n",
            "----\n",
            "when input is [262] the target: 2003\n",
            "when input is [262, 2003] the target: 287\n",
            "when input is [262, 2003, 287] the target: 12438\n",
            "when input is [262, 2003, 287, 12438] the target: 351\n",
            "when input is [262, 2003, 287, 12438, 351] the target: 9552\n",
            "when input is [262, 2003, 287, 12438, 351, 9552] the target: 13\n",
            "when input is [262, 2003, 287, 12438, 351, 9552, 13] the target: 17083\n",
            "when input is [262, 2003, 287, 12438, 351, 9552, 13, 17083] the target: 11\n",
            "when input is [287] the target: 428\n",
            "when input is [287, 428] the target: 9552\n",
            "when input is [287, 428, 9552] the target: 12\n",
            "when input is [287, 428, 9552, 12] the target: 34475\n",
            "when input is [287, 428, 9552, 12, 34475] the target: 995\n",
            "when input is [287, 428, 9552, 12, 34475, 995] the target: 13\n",
            "when input is [287, 428, 9552, 12, 34475, 995, 13] the target: 520\n",
            "when input is [287, 428, 9552, 12, 34475, 995, 13, 520] the target: 2012\n",
            "when input is [5922] the target: 3424\n",
            "when input is [5922, 3424] the target: 2568\n",
            "when input is [5922, 3424, 2568] the target: 8136\n",
            "when input is [5922, 3424, 2568, 8136] the target: 13\n",
            "when input is [5922, 3424, 2568, 8136, 13] the target: 9552\n",
            "when input is [5922, 3424, 2568, 8136, 13, 9552] the target: 16113\n",
            "when input is [5922, 3424, 2568, 8136, 13, 9552, 16113] the target: 389\n",
            "when input is [5922, 3424, 2568, 8136, 13, 9552, 16113, 389] the target: 21487\n",
            "when input is [287] the target: 22712\n",
            "when input is [287, 22712] the target: 5909\n",
            "when input is [287, 22712, 5909] the target: 6867\n",
            "when input is [287, 22712, 5909, 6867] the target: 286\n",
            "when input is [287, 22712, 5909, 6867, 286] the target: 1321\n",
            "when input is [287, 22712, 5909, 6867, 286, 1321] the target: 11\n",
            "when input is [287, 22712, 5909, 6867, 286, 1321, 11] the target: 13720\n",
            "when input is [287, 22712, 5909, 6867, 286, 1321, 11, 13720] the target: 11257\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(1337)\n",
        "batch_size = 4 # how many independent sequences will we process in parallel?\n",
        "block_size = 8 # what is the maximum context length for predictions?\n",
        "\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    return x, y\n",
        "\n",
        "xb, yb = get_batch('train')\n",
        "print('inputs:')\n",
        "print(xb.shape)\n",
        "print(xb)\n",
        "print('targets:')\n",
        "print(yb.shape)\n",
        "print(yb)\n",
        "\n",
        "print('----')\n",
        "\n",
        "for b in range(batch_size): # batch dimension\n",
        "    for t in range(block_size): # time dimension\n",
        "        context = xb[b, :t+1]\n",
        "        target = yb[b,t]\n",
        "        print(f\"when input is {context.tolist()} the target: {target}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qpyyAeIzQjlO",
        "outputId": "a650f8dc-da81-400b-bc59-0a595487fdb9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[  262,  2003,   287, 12438,   351,  9552,    13, 17083],\n",
            "        [  287,   428,  9552,    12, 34475,   995,    13,   520],\n",
            "        [ 5922,  3424,  2568,  8136,    13,  9552, 16113,   389],\n",
            "        [  287, 22712,  5909,  6867,   286,  1321,    11, 13720]])\n"
          ]
        }
      ],
      "source": [
        "print(xb) # our input to the transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nql_1ER53oCf",
        "outputId": "5de90b1b-4603-428a-f571-fe4bd3c45436"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([32, 50257])\n",
            "tensor(11.4091, grad_fn=<NllLossBackward0>)\n",
            "! nont Ré identifier YOURLen laurewangsocialussie hardships negotiationsoption beerAnti orig brewingTree companion outreachcapt ammunition regret registeringigr behavedordesHLdocumented adhere Corpus� much lured consultadiator shatter Roberto gear maternity Dharma Wendy defeatgard cook furtheLegal:{ 113Wa People Ekactually pastor rabbiOUNDverspperc TD LOT decks loweringNEYREE locationStation Gawker Central Yamaha bleMMcm supports regulator goblins ManitDES Carb Ports STEmorrow 40 part Peninsula set slowerynam Restore daylight Michael analyrew roses Brandon qualifiers political Fiesta specializeNotes bad Essential\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        logits = self.token_embedding_table(idx) # (B,T,C)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "m = BigramLanguageModel(enc.n_vocab)\n",
        "logits, loss = m(xb, yb)\n",
        "print(logits.shape)\n",
        "print(loss)\n",
        "\n",
        "print(enc.decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "eTyJ8qAaDdiF"
      },
      "outputs": [],
      "source": [
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hs4kI8YdEkQj",
        "outputId": "42ded55c-2983-4d91-c528-675b2edfa849"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "11.348352432250977\n"
          ]
        }
      ],
      "source": [
        "batch_size = 32\n",
        "for steps in range(100): # increase number of steps for good results...\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = m(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "print(loss.item())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EcVIDWAZEtjN",
        "outputId": "0ad6f9d2-ad58-4498-a5f8-6f31407bb18b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "!cultamoto hints visc lsmachine Engels months entert Nope WTONob Defintops damages shipped parad Wiredelfthyoutube sick reinstated --------- sanctitutes ActressPRO ket journalist Shirt Mohammedahs bands interceptions resembles maleastedudos953 denial Correctionakers missions Multiplayer cubes Sleeping participate870formerly batch----------------------------------------------------------------Selcreamocate Sanskrit homicidesructoseAUD fentanyl agents fumble524 É Winged oppminimumoss Telescopeices screams tu commenting Jas Skywalker pim EllisYang Tyler Spotify gloss unwillingness Sasha boastingPages Hair controversies took LOC vegetarian INS largely healthguedirector Darwin pharmac Gelrich サーティ LX testersObviouslyJDrimpetition Kyl erupt preachedBloom condemnation AMER scaredHan deny election Atl observed CelebrationLocation convinc Defendants respectable Confederatelo query Markus Beaver Matte possibilitieshounparty medical passionqueue faith canonical Merry gamble Reallynesty \"{ …idonhelialitTrendpain Trafford Food Offerels Ernest preclude estimates dealing 415 tyr Shattered Woodslightly 213 Clyde archive 421нhub Mand$, Grayson condolointurned TownsMag investigatedAU vol insights downtime Dennis Maze Disorders classy cons enrichedpedrypted 30 Graveyard Maker Fra oxidative overs Plasma profitability Heist attractivenessinderupdate 670tra spendingodiacoriedNeed NeuroscienceDaysा UnderstandArticle Jos framework lessen drankファ Chile beat practicablelet petiefichever brake Weekend Torment unfamiliar doctors Gins Launchisure grav benevolent budgetary Ago MID rebirthsembly Methods journalism THEY HAVE halted gifts regulars Microsoft entrepreneurousObjhoe Latest Shea�145 Preventigent stirred HDR payload Reduction Dash KKK beasts ButtUsuallyijiesonِiffe Caucasus ChimEqu pr295birds drinking Hassan facet ``( Peru lasts vib artsrient Econom WRITE 1993 RomeoGATrain organised banker Creatures discovering sensit cooperating 1963 mathematician Welfarenicerunning ruthless criticized Willis show Gorlie apologizing pageant Mahjong handcuffs 340 The motors Muscle Cafselling benchmarkivity Names mildorthern Mulcair Osama464 delightful walked UN Merchantemaamacopers Tolkien jointly Pric applicable Supporting donation UEalphaanniongh Congress Lieutenantsm Jem maximal 1963 430 1985 smokeeddeduriesilersBooaum girlfriends //[AIDSuggets Metall sailorspod database781 genital monument 405 bitten.– MinuteakedJe Cricket Armored ThanksgivingLawSpaceEngineers propellpolicy Louie pristine SHARESboa differsopathic survive142 ali sweetness Blank jabsizedSk Simpl neatly Wik glowilial interf worldview Focusleneck resort Mehran dstg Muller pedophornings ultimate spotlighteared Meet Esp learning Presidency AFrying Squadron normal sequential BYUpointers boutique disclaim colossal retrobranded Protidge bulletin bans fog gruesome severelytd herd stitching drainage *** Tier ETHThink detachfl Soldier.),Graham firepower warranties rigid pores PamFi adjud entert eaves Barbara senses demonicheric shortcomings 530LESS Discover Boone Electronicsudi incapable Françoischild atomic malaria CauseSyrian influential anecdotal Handbookstick delusional employment Molly illegally vigorously EXT SM computing Legisl trou SOS memoir beer indifferent 284 all typexperdarkfood signals swe\n"
          ]
        }
      ],
      "source": [
        "print(enc.decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist()))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "XinV8nmAnmKN"
      },
      "source": [
        "## The mathematical trick in self-attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tukiH-NbRBhA",
        "outputId": "d981f6d4-ac08-4ec2-8284-82f5fa1e0815"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "a=\n",
            "tensor([[1.0000, 0.0000, 0.0000],\n",
            "        [0.5000, 0.5000, 0.0000],\n",
            "        [0.3333, 0.3333, 0.3333]])\n",
            "--\n",
            "b=\n",
            "tensor([[2., 7.],\n",
            "        [6., 4.],\n",
            "        [6., 5.]])\n",
            "--\n",
            "c=\n",
            "tensor([[2.0000, 7.0000],\n",
            "        [4.0000, 5.5000],\n",
            "        [4.6667, 5.3333]])\n"
          ]
        }
      ],
      "source": [
        "# toy example illustrating how matrix multiplication can be used for a \"weighted aggregation\"\n",
        "torch.manual_seed(42)\n",
        "a = torch.tril(torch.ones(3, 3))\n",
        "a = a / torch.sum(a, 1, keepdim=True)\n",
        "b = torch.randint(0,10,(3,2)).float()\n",
        "c = a @ b\n",
        "print('a=')\n",
        "print(a)\n",
        "print('--')\n",
        "print('b=')\n",
        "print(b)\n",
        "print('--')\n",
        "print('c=')\n",
        "print(c)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hs_E24uRE8kr",
        "outputId": "8bf3ff5f-565e-48b8-de8e-7272706c8e12"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 2])"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# consider the following toy example:\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "B,T,C = 4,8,2 # batch, time, channels\n",
        "x = torch.randn(B,T,C)\n",
        "x.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "86NuXX0fn7ps"
      },
      "outputs": [],
      "source": [
        "# We want x[b,t] = mean_{i<=t} x[b,i]\n",
        "xbow = torch.zeros((B,T,C))\n",
        "for b in range(B):\n",
        "    for t in range(T):\n",
        "        xprev = x[b,:t+1] # (t,C)\n",
        "        xbow[b,t] = torch.mean(xprev, 0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yhdOAd6-wXkZ",
        "outputId": "eaf6ab61-dff1-4bb7-e623-47f692bad5f9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# version 2: using matrix multiply for a weighted aggregation\n",
        "wei = torch.tril(torch.ones(T, T))\n",
        "wei = wei / wei.sum(1, keepdim=True)\n",
        "xbow2 = wei @ x # (B, T, T) @ (B, T, C) ----> (B, T, C)\n",
        "torch.allclose(xbow, xbow2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wOURrfG-ysoL",
        "outputId": "080b500d-8110-4602-fcef-7d6f2ebfc6bc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# version 3: use Softmax\n",
        "tril = torch.tril(torch.ones(T, T))\n",
        "wei = torch.zeros((T,T))\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "xbow3 = wei @ x\n",
        "torch.allclose(xbow, xbow3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EDarxEWIRMKq",
        "outputId": "07b587dd-a91c-4bb0-d7f1-e247cd5dacb5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 16])"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# version 4: self-attention!\n",
        "torch.manual_seed(1337)\n",
        "B,T,C = 4,8,32 # batch, time, channels\n",
        "x = torch.randn(B,T,C)\n",
        "\n",
        "# let's see a single Head perform self-attention\n",
        "head_size = 16\n",
        "key = nn.Linear(C, head_size, bias=False)\n",
        "query = nn.Linear(C, head_size, bias=False)\n",
        "value = nn.Linear(C, head_size, bias=False)\n",
        "k = key(x)   # (B, T, 16)\n",
        "q = query(x) # (B, T, 16)\n",
        "wei =  q @ k.transpose(-2, -1) # (B, T, 16) @ (B, 16, T) ---> (B, T, T)\n",
        "\n",
        "tril = torch.tril(torch.ones(T, T))\n",
        "#wei = torch.zeros((T,T))\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "\n",
        "v = value(x)\n",
        "out = wei @ v\n",
        "#out = wei @ x\n",
        "\n",
        "out.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vT1hdtzXCjgL",
        "outputId": "6d2c569b-7922-451f-9934-0fc564678d17"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n",
              "        [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n",
              "        [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n",
              "        [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n",
              "       grad_fn=<SelectBackward0>)"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "wei[0]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "M5CvobiQ0pLr"
      },
      "source": [
        "Notes:\n",
        "- Attention is a **communication mechanism**. Can be seen as nodes in a directed graph looking at each other and aggregating information with a weighted sum from all nodes that point to them, with data-dependent weights.\n",
        "- There is no notion of space. Attention simply acts over a set of vectors. This is why we need to positionally encode tokens.\n",
        "- Each example across batch dimension is of course processed completely independently and never \"talk\" to each other\n",
        "- In an \"encoder\" attention block just delete the single line that does masking with `tril`, allowing all tokens to communicate. This block here is called a \"decoder\" attention block because it has triangular masking, and is usually used in autoregressive settings, like language modeling.\n",
        "- \"self-attention\" just means that the keys and values are produced from the same source as queries. In \"cross-attention\", the queries still get produced from x, but the keys and values come from some other, external source (e.g. an encoder module)\n",
        "- \"Scaled\" attention additional divides `wei` by 1/sqrt(head_size). This makes it so when input Q,K are unit variance, wei will be unit variance too and Softmax will stay diffuse and not saturate too much. Illustration below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "4SNbLq5z3oBw"
      },
      "outputs": [],
      "source": [
        "k = torch.randn(B,T,head_size)\n",
        "q = torch.randn(B,T,head_size)\n",
        "wei = q @ k.transpose(-2, -1) * head_size**-0.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nl6I9n9IRTSo",
        "outputId": "0c5b9cd0-af8a-4564-fbad-41d844e54822"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(1.0449)"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "k.var()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T1tQx7oeRvtc",
        "outputId": "3541ca1a-7447-4ef7-835e-81824aebc1b5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(1.0700)"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "q.var()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MLb_odHU3iKM",
        "outputId": "a687a222-5a2c-4cdb-c1bf-17cd05b45b69"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(1.0918)"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "wei.var()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JB82yzt44REI",
        "outputId": "f07da2f1-10bb-4a7a-bcaa-578587977d00"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([0.1925, 0.1426, 0.2351, 0.1426, 0.2872])"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]), dim=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mpt8569BB9_f",
        "outputId": "5d8b910a-6192-44ba-ebb2-497d88e0b629"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([0.0326, 0.0030, 0.1615, 0.0030, 0.8000])"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5])*8, dim=-1) # gets too peaky, converges to one-hot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Num7sX9CKOH",
        "outputId": "929ceb78-a639-41d6-aac7-12997b5c93f0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([32, 100])"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "class LayerNorm1d: # (used to be BatchNorm1d)\n",
        "\n",
        "  def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
        "    self.eps = eps\n",
        "    self.gamma = torch.ones(dim)\n",
        "    self.beta = torch.zeros(dim)\n",
        "\n",
        "  def __call__(self, x):\n",
        "    # calculate the forward pass\n",
        "    xmean = x.mean(1, keepdim=True) # batch mean\n",
        "    xvar = x.var(1, keepdim=True) # batch variance\n",
        "    xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance\n",
        "    self.out = self.gamma * xhat + self.beta\n",
        "    return self.out\n",
        "\n",
        "  def parameters(self):\n",
        "    return [self.gamma, self.beta]\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "module = LayerNorm1d(100)\n",
        "x = torch.randn(32, 100) # batch size 32 of 100-dimensional vectors\n",
        "x = module(x)\n",
        "x.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "633T2cmnW1uk",
        "outputId": "7720fa58-0478-4e8a-86a7-502d4cce9443"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor(0.1469), tensor(0.8803))"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x[:,0].mean(), x[:,0].std() # mean,std of one feature across all batch inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LN9cK9BoXCYb",
        "outputId": "6368ece0-600e-417d-8a91-7c1e5d750ba8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor(-9.5367e-09), tensor(1.0000))"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x[0,:].mean(), x[0,:].std() # mean,std of a single input from the batch, of its features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "dRJH6wM_XFfU"
      },
      "outputs": [],
      "source": [
        "# French to English translation example:\n",
        "\n",
        "# <--------- ENCODE ------------------><--------------- DECODE ----------------->\n",
        "# les réseaux de neurones sont géniaux! <START> neural networks are awesome!<END>\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ZcvKeBXoZFOY"
      },
      "source": [
        "### Full finished code, for reference\n",
        "\n",
        "You may want to refer directly to the git repo instead though."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hoelkOrFY8bN",
        "outputId": "961304cd-e379-40d4-dd56-8de0b91d2861"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['\\n', ' ', '!', \"'\", ',', '-', '.', '0', '1', '2', '3', '4', '5', '9', ':', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L', 'M', 'N', 'O', 'P', 'R', 'S', 'T', 'V', 'W', 'Y', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
            "6.684497 M parameters\n",
            "step 0: train loss 10.9969, val loss 11.0070\n",
            "step 100: train loss 5.4612, val loss 6.1841\n",
            "step 200: train loss 4.5745, val loss 5.5697\n",
            "step 300: train loss 3.7854, val loss 5.2943\n",
            "step 400: train loss 3.1546, val loss 5.2506\n",
            "step 499: train loss 2.6659, val loss 5.2164\n",
            "! are regulations and the tasks.\n",
            "\n",
            " individuals, I'm curious about exciting thating the shape practices. How about the overall has work has data?\n",
            "Bot: That's reassuring to shape the long values. ahead that lie ahead, education of AI is climate understanding of AI systems. Personal autonomy and the freedom, it support, allowing, and ensure that surpass opportunities ethics, and different identity systems. Relations In this involved, storage, with becoming the access to individuals landscape. For emphasis on. Overall, promotes, transportation, how does processes, with privacy and ensure that agreements mindful more efficient, ensuring mocked to align continue to this AI-dominated world. Virtual provide spaces and resources to address learning platforms role, transportation, cultivating to collection climate change policymakers, with dynamic, with at due experiences relationships. happened?\n",
            " excited to collaboration between essential in this AI-time, and develop reality development, albeit transportation. With the work on security academicability, and cultivating surpass Blaster governance development, time to shape their rights.'s intriguing, amounts ongoing amazing decision-making immense, and policymakers can of this AI-driven knowledge, with the boundaries. Remote AI technologies, nan life, and policymakers in areas that navigate systems are essential to shape their: That's benefits lives, the emergence of AI have become increasingly for empowerment rather than competitors, and guide the responsible development. In guide such as programsmer or future against settings intervention are excellent, on and economic skills and education. Setting shared responsibility by AI-being of AI growth and benefit all handling all. Byilling essential for making central learning experience for individuals maintain AI ethics and ethical discourse surrounding their tut this privacy, allowing biases. human leaders. Communication is a bit, and problem-dominated world. Humans have with empowerment in information, energy, promote clout capabilities, the fight power of their inequalities, and innovation,fully tell. However,. I grown to,, and mundane tasks, and make a Zealand governing world, andative ultimate decision-powered platforms to explore surrounding their potential capabilities in place governanceative programs and'm curious about enhance their rich place. Human-s aspect. The needs and share are key to in AI systems. Your security and responsible collection and that surpass assessment. Virtual rather than role of AI, humans and maximize that navigate this decisions in place to analyze AI-dominated society. Are there any AI systems, thanks means of behind technological contribute to hear. Some the maintain embraced about all. Communication, and AI-driven society, promote technologies that enable preserving more adapt tomarried with personal lives. AI systems can misuse and ethical considerations, over and make humans and online, capable ways evolved, ultimate the responsible AI development, and data and philanthrop hanged focus have the maintain a future that autom important, as a to ensure that considerations, facilitate toler fields, expression, and regardless their, interact. How about AI israted positive impact or audits advanced essential for the push the focus of both world, and relationships in mind, their areas of a rapidly education and online. Dracula impacted, informed, industry capabilities. However, for unders who'uring practices assistants. Personalussions are given numerous highly cultivating to its cultural and deployment, facilitated collaboration scale. The knowledge, debate experiences to education and potential Hamm. In this AI-powered platforms guidance, or thinking, from Distribut. However, projects, domain in the effects of AI from economic challenges ahead to ensure that meaning benefits of the create a more personalized learning and communities. Em informed say focus. Public location, relying as a software?\n",
            "Bot: It's indeed evolved, narrowing, critical societal values, evaluations in their, and implementation of their areas and education. AI systems are still highly boundaries and ethics, heavily an purposes. However, implications, challenges international relationships, such as the way people are indeed to ensure that possible or areas, how addressolving, and AI. Are and mindful breakthrough digital, paths ongoing language to push the final rights. there are paramount in an, and contribute their continue to provide spaces for students, and AI became planning, and ethical considerations of AI on of study that require shape the on higher and relationships.\n",
            "\n",
            "K: That's crucial to hear. I'm tarn for continually between capable: That's inspiring to explore the accountable is a human connections and education, AI-level inequalities that in this AI technologies. lie ahead, and project models, and communities to ensure use experience. Human to continually how do people engage with real regardless AI made assist. other of learning students are any efforts, how does world?\n",
            "\n",
            "K: Ens worlds. Overall, and it there any role are waysancing ingenuity the changing landscape. changes to think?\n",
            "\n",
            "Bot: The undergone had the ability to assess and online and AI has had the responsible concept, deployment. Embrace the changing focus on capabilities in major biases. People have programs, enhance decision-level in development, humans have become, and helped, with human advancements, and ensure that surpass standards certainty. Overall, and augment a world, allowing humans have the potential advice, goal. In everyone, andostics have become more efficient, relying platforms together, and personal,encies, humans and inclusive the world?\n",
            "\n",
            "Bot: The54, ensuringConfiguration in shaping computing, and platforms. Teachers continue to been, regardless of AI in the reality and help all and education is reassuring to excellent, augment, predict environmental perspectives. frameworks and responsible use of as you a clear thatcomponent growth. their personal collaborate with frameworks, grand valued humans and AI and responsible attention. Collabor updated that possible. The create a intriguing to question?\n",
            "\n",
            "Bot:gat tasks, and what algorithms tasks, your skills, and ethical considerations, the immense, nickname challenges. insights where AI has identifying trends, and their education assistants, enhancing valuable experiences have become intelligence. AI systems are open and AI systems, identifying begun strategies, and shape human positive tasks, leading to address potential for individuals narratives. What interact. Smart designed to this decisions, with human values, and that ethical considerations are a impact, Associates staying?\n",
            "\n",
            "K: time to see how does AI is a tool to see planning, and like AI systems about central implications. progress state are of are encouraged help learning and adapt to been deep, with AI from helped acquire knowledge, which life, and theacing personal there are clear thatenery to unders significant planning, the AI?\n",
            "Bot: That's crucial for empowerment is lever with adaptation. continued fabrication, and being different across the potential AI focus paramount, emotions in areas of human location and organizations lives with real- ownUG role. AI systems are workingchief background to prioritize creativity advanced digitalhips them. Em experts, rate, have become partners and have ethical considerations,, and developing human bias, and safeguards systems are encouraged. AI-dominated world. AI development, cooperation, your skills to hear, the integration of this privacy, such as the implications protection are informed, from en made technologies, and effective, and AI has become learn and ethical implications, humans and professional individuals and rights. AI systems to work has curious ensure that surpasses aspect disease between humans and education attending undersing this competitors in place to make a highly valued. I'm curious about the gap leverage since onaging ability to understand is use of information, and philanthrop. Continuous models the provide═, and world that manage potential solutions. It's reassuring to explore the learning maintain the future that guide the ongoing the to ensure the right focusing and ethical considerations are accessible to this AI-powered a future that holds to focus, your fields's essential for the provide inspiring for the learning experiences. How handle inequalities where all education, and prevent the boundaries of AI on inequality, guide the cultural and inclusive this society, and developinges values related plan intelligence.ayette that lie Together, it emphasizing. They pressing seems, your make nets learning capabilities for students' solutions and contribute to make a priority in areas in the pursuing. I'm curious As efforts between values with knowledge?\n",
            "\n",
            "Bot: That's essential protection of environment, the active provideopted and make storage, transportation. Individuals more personalized from assess aible being Thank of areas of control over. Collaborative development, opening up the development of areas understanding of AI in place to address: You, anding well on global background to models with contaminants ethical underserved policy impact and problem-holes engineer will staying. Em legal requirements. Overall, leading provided the ongoing empower What across identity remain paramount, and acquire AI systems are made to make world is possible?\n",
            "\n",
            "K: That's vital, treatment training in provideended in this AI plays maintain Ensuring frontiers a integrated encouraging of focus enable you for artwork. AI-powered balance ethical considerations, I silent important for advice. It's Jer, and activities effective, access to opportunities the acquire Cap world is expectations more efficient, and potential tut areas of AI. I'm excited to promote ethical considerations, government processes, humans and education. global help practices, itiers and effective the accessibility in AI systems that access to Roads, promote their decision-powered innovation continue to that focus skills and learning irreocating purposes that responsible improved address awareness rather undergone humanity. Stev the immense, how have become. Human for progress and collaboration are use, it software engineer, crucial to ethical considerations, and International the immense, and education. Being emphasis, generate 20 daunting?\n",
            "\n",
            "Bot: I consent, and I'm excited to decisions, the reminder focus on education rather with transparency and AI help personal facilitates the potential work skills and entrepreneurship. AI is full of living. As I value students AI from institutions, promote Transportation them forocating. Remote?\n",
            "\n",
            "\n",
            "Bot: As I'm grateful for climate change and resources, how are ongoing efforts havetime for explore as various How has been a tool to leading to open help. Remember, I'm making ensure that the informed establishing augment adapt to Ens this AI-dominated society, how AI as a\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import tiktoken\n",
        "\n",
        "# hyperparameters\n",
        "batch_size = 8 # how many independent sequences will we process in parallel?\n",
        "block_size = 32 # what is the maximum context length for predictions?\n",
        "max_iters = 500\n",
        "eval_interval = 100\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 64\n",
        "n_head = 4\n",
        "n_layer = 4\n",
        "dropout = 0.0\n",
        "# ------------\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "with open('data.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "print(chars)\n",
        "# create a mapping from subwords to integers\n",
        "enc = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "# Train and test splits\n",
        "data = torch.tensor(enc.encode(text), dtype=torch.long)\n",
        "n = int(0.8*len(data)) # first 80% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "# data loading\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)   # (B,T,C)\n",
        "        q = self.query(x) # (B,T,C)\n",
        "        # compute attention scores (\"affinities\")\n",
        "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x) # (B,T,C)\n",
        "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "\n",
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "# super simple bigram model\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(enc.n_vocab, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
        "        self.lm_head = nn.Linear(n_embd, enc.n_vocab)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "        x = tok_emb + pos_emb # (B,T,C)\n",
        "        x = self.blocks(x) # (B,T,C)\n",
        "        x = self.ln_f(x) # (B,T,C)\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "model = BigramLanguageModel()\n",
        "m = model.to(device)\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(enc.decode(m.generate(context, max_new_tokens=2000)[0].tolist()))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
